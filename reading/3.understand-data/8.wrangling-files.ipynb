{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data from plain text source file with different formats and encodings and potentioal large size into `python DataFrame` and indentify its granularity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Plain Text Format\n",
    "Easy to read with text editor such as **Sublime, Vim, Emacs**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Delimited Format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. each line represents a record, delimited by newline `\\n` or `\\r\\n`\n",
    "2. within each line: special character to separate data values\n",
    "    - comma: csv\n",
    "    - tab: tsv\n",
    "    - white space(s), colons\n",
    "3. the first line contains the names of tables columns or features\n",
    "\n",
    "People often confuse CSV and TSV files with spreadsheets. This is in part because most spreadsheet software (like Microsoft Excel) will automatically display a CSV file as a table in a workbook. Behind the scenes, Excel looks at the file format and encoding just like we’ve done in this section. However, Excel files have a different format than CSV and TSV files, and we need to use different pandas functions to read these formats into Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filename extension\n",
    "indicator, expections, and suggestions\n",
    "\n",
    "It’s good practice to inspect the contents of the file before loading it into a data frame. If the file is not too large, you can open and examine it with a plain text editor. Otherwise, you view a couple of lines using .readline() or shell commands."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pathlib\n",
    "the built-in `pathlib` library has a useful `Path` object to specify paths to files and folders that work across platforms\n",
    "\n",
    "Paths are tricky when working across different operating systems (OS). For instance, a typical path in Windows might look like C:\\files\\data.csv, while a path in Unix or MacOS might look like ~/files/data.csv. Because of this, code that works on one OS can fail to run on other operating systems.\n",
    "\n",
    "The pathlib Python library was created to avoid OS-specific path issues. By using it, the code shown here is more portable—it works across Windows, MacOS, and Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jasonwu/GitHub/ds/data-100/lec/lec06/data/inspections.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "p = Path('/Users/jasonwu/GitHub/ds/data-100') / 'lec' / 'lec06' / 'data' / 'inspections.csv'\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(filepath, n=5, width=-1):\n",
    "    '''Prints the width characters of first n lines of filepath'''\n",
    "    with filepath.open() as f:\n",
    "        for _ in range(n):\n",
    "            (print(f.readline(), end='') if width < 0  \n",
    "             else print(f.readline()[:width]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"business_id\",\"score\",\"date\",\"type\"\n",
      "\n",
      "19,\"94\",\"20160513\",\"routine\"\n",
      "\n",
      "19,\"94\",\"20171211\",\"routine\"\n",
      "\n",
      "24,\"98\",\"20171101\",\"routine\"\n",
      "\n",
      "24,\"98\",\"20161005\",\"routine\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head(p, width=65)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fixed-width Format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed-width format (FWF) does not use delimiters to separate data values. Instead, the values for a specific field appear in the exact same position in each line\n",
    "\n",
    "values are aligned from one row to the next, some of the values seem to be squished together\n",
    "`codebook` provide the position information and some basic checks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a file contains 200 thousand lines and over 280 million characters so, on average, there are about 1200 characters per line. This might be why they used a fixed-width rather than a CSV format. Think how much larger the file would be if there was a comma between every field!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas.read_fwf to read a subset of a large fixed-width file\n",
    "```\n",
    "colspecs = [(0,6), (14,29), (33,35), (35, 37), (37, 39), (1213, 1214)]\n",
    "varNames = [\"id\", \"wt\", \"age\", \"sex\", \"race\",\"type\"]\n",
    "dawn = pd.read_fwf('data/DAWN-Data.txt', colspecs=colspecs, \n",
    "                   header=None, index_col=0, names=varNames)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hierarchical Formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested form: JSON, XML, HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loosely Formatted Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loosely format: \n",
    "organizational pattern, but no delimiters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples:\n",
    "Web logs, contains information such as the date and time and type of request made to a Web site:\n",
    "```\n",
    "169.237.46.168 - -\n",
    "[26/Jan/2004:10:47:58 -0800]\"GET /stat141/Winter04 HTTP/1.1\" 301 328\n",
    "\"http://anson.ucdavis.edu/courses\"\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 1.1.4322)\"\n",
    "```\n",
    "Wireless device log: reports the timestamp, identifier, location of the device, and the signal strengths that it picks up from other devices. This information uses a combination of formats: _key=value pairs, semicolon delimited, and comma delimited values_\n",
    "```\n",
    "t=1139644637174;id=00:02:2D:21:0F:33;pos=2.0,0.0,0.0;degree=45.5;\n",
    "00:14:bf:b1:97:8a=-33,2437000000,3;00:14:bf:b1:97:8a=-38,2437000000,3;\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: file format and strucrue of data are different\n",
    "1. file format: stored types\n",
    "2. data structure: mental represnetation \\\n",
    "pandas DataFrame, SQL relation, text data, hierachical and binary file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### catagories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used the term ‘plain text’ to broadly cover formats that can be viewed with a text editor. However, a plain text file may have different encodings, and if we don’t specify the encoding correctly, the values in the data frame might contain gibbersih. We give an overview of file encoding next.\n",
    "\n",
    "Computers only recognize 0 and 1, we need a map to translate 0, 1 digits to human texts\n",
    "\n",
    "1. ASCII\n",
    "2. Latin-1 (ISO-8859-1)\n",
    "3. UTF-8 (backwards compatible with ASCII)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### figuring out encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. check the data's documentation\n",
    "2. meta data\n",
    "3. `chardet` package, `detect()` function infers the confidence of a file's encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                 Encoding   Confidence\n",
      "wrangling.ipynb           utf-8      0.99\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "line = '{:<25} {:<10} {}'.format\n",
    "\n",
    "# for each file, print its name, encoding & confidence in the encoding\n",
    "print(line('File Name', 'Encoding', 'Confidence'))\n",
    "\n",
    "for filepath in Path().glob('*'):\n",
    "    result = chardet.detect(filepath.read_bytes())\n",
    "    print(line(str(filepath), result['encoding'], result['confidence']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify the encoding when loading data\n",
    "\n",
    "`pd.read_csv(filename, encoding=[])`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. os\n",
    "begin our data work by making sure the files are of manageable size \n",
    "\n",
    "                `os.path.getsize(filepath)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Command Line Interface (CLI)\n",
    "**_commmands_** available in a shell interpreter (sh, bash, zsh, perform oparations on files with their own syntax, build-in commands, and language)\n",
    "\n",
    "                `command -options arguments`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation\n",
    "- if you need to record what you did\n",
    "\n",
    "Error reduction\n",
    "- if you want to reduce typographical errors and other simple but potentially harmful mistakes\n",
    "\n",
    "Reproducibility\n",
    "- if you need to repeat the same process in the future or you plan to share your process with others, you have a record of your actions\n",
    "\n",
    "Volume\n",
    "- if you have many repetitive operations to perform, the size of the file you are working with is large, or you need to perform things quickly, then CLI tools can help.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ls`\n",
    "\n",
    "`-l`: provide extra information about each file; \n",
    "`-h`: provide filesizes in a more human-readable format; \n",
    "`-L`: Symbolic links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`du`\n",
    "\n",
    "dest usage, shows the size in units called blocks\n",
    "\n",
    "`-s`: show the file sizes for both files and folders\n",
    "`-h`: display quantities in the standard KiB, MiB, GiB format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wc`\n",
    "\n",
    "word count (line, word, character)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`head`, `tail`\n",
    "\n",
    "displays 10 lines of a file by default\n",
    "\n",
    "`-n #` or `-#`: # lines of file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cat`\n",
    "\n",
    "concatinate: print the entire file’s contents\n",
    "\n",
    "**take care when using this command, as printing a large file can cause a crash**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file`\n",
    "\n",
    "help use determine a file’s encoding\n",
    "\n",
    "```\n",
    "file -I data/*\n",
    "\n",
    "data/DAWN-Data.txt:   text/plain; charset=us-ascii\n",
    "data/businesses.csv:  application/csv; charset=iso-8859-1\n",
    "data/co2_mm_mlo.txt:  text/plain; charset=us-ascii\n",
    "data/inspections.csv: application/csv; charset=us-ascii\n",
    "data/legend.csv:      application/csv; charset=us-ascii\n",
    "data/violations.csv:  application/csv; charset=us-ascii\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with large files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In scientific domains like astronomy, where telescopes capture images of space that can be petabytes in size. While not quite as big, social media giants, health care providers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Subset The Data\n",
    "Either select a specific part of it (e.g., one day’s worth of data), or we can randomly sample the data set.\n",
    "\n",
    "Simple but may lose many of the benefits like rare events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Data System\n",
    "Relational database management systems (RDBMS) are specifically designed to store large data sets\n",
    "\n",
    "Downside: \n",
    "- require a separate server for the data that needs its own configuration\n",
    "- SQL is less flexible in what it can compute than Python, which becomes especially relevant for modeling.\n",
    "\n",
    "Hybrid Approach: \n",
    "SQL: subset -- aggregate -- sample\n",
    "Python: more sophisticated analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Distributed Computing System\n",
    "MapReduce, Spark, or Ray\n",
    "\n",
    "These systems work best on tasks that can be split into many smaller parts where they divide up data sets into smaller pieces and run programs on all of the smaller data sets at once. These systems have great flexibility and can be used in a variety of scenarios. Their main downside is that they can require a lot of work to install and configure properly because they are typically installed across many computers that need to coordinate with each other.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data into Memory (RAM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Python code requires the use of RAM, no matter how short the code is. A computer’s RAM is typically much smaller than its disk storage.\n",
    "\n",
    "Multiple | Notation | Number of Bytes\n",
    "--- | --- | --- |\n",
    "Kibibyte|KiB|1024\n",
    "Mebibyte|MiB|1024²\n",
    "Gibibyte|GiB|1024³\n",
    "Tebibyte|TiB|1024⁴\n",
    "Pebibyte|PiB|1024⁵\n",
    "\n",
    "You also see the typical SI prefixes used to describe size—kilobytes, megabytes, and gigabytes, for example. Unfortunately, these prefixes are used inconsistently. Sometimes a kilobyte refers to 1000 bytes; other times, a kilobyte refers to 1024 bytes. To avoid confusion, we stick to kibi-, mebi-, and gibibytes which clearly represent multiples of 1024.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, reading in a file using pandas usually requires at least five times the available memory as the file size. Memory is shared by all programs running on a computer, including the operating system, web browsers, and Jupyter notebook itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Shape and Granularity "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape: quantifies the table’s rows and columns\n",
    "\n",
    "granularity: describe what each row in the table represents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Key and Foreign Keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Field as UID: `len(DataFrame) == len(DataFrame.groupby(id).unique())`\n",
    "\n",
    "Two or More (combination of multiple fields):\n",
    "    `DataFrame.groupby([field1, field2]).size().sort_values()`\n",
    "\n",
    "Since we have identified primary and foreign keys for them, we can potentially join these tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reflect the sampling scheme and be representative of the population of all drug-related ER visits in a year, weights are provided. We must apply the weight to each record when we compute summary statistics, build histograms, and fit models. (The wt field contains these values).\n",
    "\n",
    "The weights take into account the chance of an ER visit like this one appearing in the sample. By “like this one” we mean a visit with similar features, such as the visitor age, race, visit location, and time of day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is critical to include the survey weights in your analysis to get data that represents the population at large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Qustions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does a record represent?\n",
    "\n",
    "2. Do all records in a table capture granularity at the same level? Sometimes a table contains additional summary rows that have a different granularity, and you want to use only those rows that are at the right level of detail.\n",
    "\n",
    "3. How was the aggregation performed, what kinds of it is performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrangling techniques in this chapter help us bring data from a source file into a data frame and understand its structure. Once we have a data frame, further wrangling is needed to assess and improve quality and prepare the data for analysis.``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
